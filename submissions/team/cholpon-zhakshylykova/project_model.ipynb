{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model \n",
    "\n",
    "There are many ways to model a time series in order to make predictions. The most popular ways include:\n",
    "\n",
    "- Moving average.\n",
    "- Exponential smoothing.\n",
    "- Double exponential smoothing.\n",
    "- Triple exponential smoothing.\n",
    "- Seasonal autoregressive integrated moving average (SARIMA.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average\n",
    "- in general is used to see trends of the data.\n",
    "- the next oversvation is the mean of all past observations. \n",
    "- By changing the value of the window size you can smooth the data and maybe observe some trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average\n",
    "window_size = 24\n",
    "data['EnergyConsumption_smoothed'] = data['EnergyConsumption'].rolling(window=window_size).mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(data[\"Timestamp\"], data['EnergyConsumption'], label='Original EnergyConsumption', color='blue')\n",
    "plt.plot(data[\"Timestamp\"], data['EnergyConsumption_smoothed'], label=f'Moving Average (Window={window_size})', linestyle='--', color='red')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('EnergyConsumption')\n",
    "plt.title('Original vs Moving Average')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential smoothing\n",
    "- here applies the similar logic as in the moving average, except less importance is given to the next data as we move further from the present. \n",
    "- math. equation: y = AXt + (1-A)Yt-1, t > 0\n",
    "    - A is a smoothing factor between 0 and 1; it determines how fast the weight decreases for the previous observations. \n",
    "    - the closer is A to zero the smoother the line will be and will approach the average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double exponential smoothing\n",
    "- used when there is a trend in the time series\n",
    "- exponential smoothing is used twice: \n",
    "    - math. equations: \n",
    "    y = AXt + (1-A)(Yt-1 + Bt-1)\n",
    "    Bt = ß(Yt -Yt-1) + (1-ß)Bt-1\n",
    "    ß: trend smoothing factor, takes value between 0 and 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple exponential smoothing\n",
    "\n",
    "- This method extedns the double exponential smoothing by adding a seasonal exponential factor. \n",
    "- It is useful when we see seasonality in time series. \n",
    "- y(gamma) is the seasonal smoothing factor and L is the length of the season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal autoregressive Integrated Moving average Model (SARIMA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA\n",
    "\n",
    "#follow along the tutorial https://builtin.com/data-science/time-series-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ARMA model can explain the relationship of time series with both random noise (moving average) and itself at a previous step (autogregression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"]  = [10,7.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean average percentage error (MAPE) should be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_date_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    \n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n",
    "    \n",
    "    #Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "            \n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "#Smooth by the previous 5 days (by week)\n",
    "plot_moving_average(data_date_indexed.EnergyConsumption, 1)\n",
    "\n",
    "#Smooth by the previous month (30 days)\n",
    "plot_moving_average(data_date_indexed.EnergyConsumption, 7)\n",
    "\n",
    "#Smooth by previous quarter (90 days)\n",
    "plot_moving_average(data_date_indexed.EnergyConsumption, 30, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exponential smoothing\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "  \n",
    "def plot_exponential_smoothing(series, alphas):\n",
    " \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True);\n",
    "\n",
    "plot_exponential_smoothing(data_date_indexed.EnergyConsumption, [0.05, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Double Exponetial smoothing\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "def plot_double_exponential_smoothing(series, alphas, betas):\n",
    "     \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "plot_double_exponential_smoothing(data_date_indexed.EnergyConsumption, alphas=[0.9, 0.02], betas=[0.9, 0.02])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "tsplot(data_date_indexed.EnergyConsumption, lags=30)\n",
    "\n",
    "# Take the first difference to remove to make the process stationary\n",
    "#data_diff = data_date_indexed.EnergyConsumption - data_date_indexed.EnergyConsumption.shift(1)\n",
    "\n",
    "#tsplot(data_diff[1:], lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m s = \u001b[32m5\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Create a list with all possible combinations of parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m parameters = \u001b[43mproduct\u001b[49m(ps, qs, Ps, Qs)\n\u001b[32m     14\u001b[39m parameters_list = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters_list)\n",
      "\u001b[31mNameError\u001b[39m: name 'product' is not defined"
     ]
    }
   ],
   "source": [
    "#Sarima\n",
    "\n",
    "#Set initial values and some bounds\n",
    "ps = range(0, 5)\n",
    "d = 1\n",
    "qs = range(0, 5)\n",
    "Ps = range(0, 5)\n",
    "D = 1\n",
    "Qs = range(0, 5)\n",
    "s = 5\n",
    "\n",
    "#Create a list with all possible combinations of parameters\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)\n",
    "\n",
    "# Train many SARIMA models to find the best set of parameters\n",
    "def optimize_SARIMA(parameters_list, d, D, s):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order\n",
    "        D - seasonal integration order\n",
    "        s - length of season\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float('inf')\n",
    "    \n",
    "    for param in tqdm(parameters_list):\n",
    "        try: \n",
    "            model = sm.tsa.statespace.SARIMAX(data_date_indexed.EnergyConsumption, order=(param[0], d, param[1]),\n",
    "                                               seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        \n",
    "        #Save best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "        \n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    #Sort in ascending order, lower AIC is better\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "result_table = optimize_SARIMA(parameters_list, d, D, s)\n",
    "\n",
    "#Set parameters that give the lowest AIC (Akaike Information Criteria)\n",
    "p, q, P, Q = result_table.parameters[0]\n",
    "\n",
    "best_model = sm.tsa.statespace.SARIMAX(data_date_indexed.EnergyConsumption, order=(p, d, q),\n",
    "                                       seasonal_order=(P, D, Q, s)).fit(disp=-1)\n",
    "\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# save the output of the summary of the model into a separate file\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msarima_summary.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m summary1:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     summary1.write(\u001b[38;5;28mstr\u001b[39m(\u001b[43mbest_model\u001b[49m.summary()))\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# save the output of the summary of the model into a separate file\n",
    "with open(\"sarima_summary.txt\", \"w\") as summary1:\n",
    "    summary1.write(str(best_model.summary()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Long short-term Memory (LSTM)\n",
    "\n",
    "#This is a deep learning model that can handle time seris data with long-term dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Check for missing or duplicated timestamps\n",
    "missing_timestamps = data['Timestamp'].isnull().sum()\n",
    "duplicated_timestamps = data['Timestamp'].duplicated().sum()\n",
    "\n",
    "print(f\"Missing timestamps: {missing_timestamps}\")\n",
    "print(f\"Duplicated timestamps: {duplicated_timestamps}\")\n",
    "\n",
    "# Ensure timestamps are properly formatted and consistent\n",
    "try:\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "    print(\"Timestamps are properly formatted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in timestamp formatting: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
